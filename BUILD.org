#+TITLE: Building AlzKB (from scratch)
#+AUTHOR: Joseph D. Romano
#+EMAIL: joseph.romano@pennmedicine.upenn.edu
#+LANGUAGE: en
#+OPTIONS: toc:nil author

* Overview
This guide will teach you the complete process of building the
Alzheimer's Knowledge Base (AlzKB). It's not a concise process, but it
is extensible to other applications of knowledge engineering. We use
the same process for our other knowledge bases (such as [[https://comptox.ai][ComptoxAI]]), so
this guide can also be used to teach you how to build your own.

The following diagram gives an overview of the build process:

#+CAPTION: Summary of how to build AlzKB
[[./img/build-abstract.png]]

1. First, you use domain knowledge to create the ontology
2. Then, you collect the data sources and use them to populate the
   ontology
3. Finally, you convert the ontology into a graph database

* 1.: Creating the AlzKB Ontology
_Important note_: Most users don't need to follow these steps, since
it is already done! Unless you want to extend AlzKB or make major
modifications to its node/edge types, you should skip to the [[Obtaining the third-party data sources][next
section]]. If you DO want to do those things, then keep reading.

AlzKB uses an OWL 2 ontology to act something like a 'template' for
the nodes and relationships in the final knowledge graph. While the
actual nodes and relationships are added automatically according to
the 'rules' defined in the ontology, the ontology itself is
constructed manually, using domain knowledge about AD. We do this
using the Protégé ontology editor. If you don't already have it,
download and install [[https://protege.stanford.edu/products.php][Protégé Desktop]] on your computer.

* 2.: Obtaining the third-party data sources
The next step is to collect the source data files that will eventually
become the nodes, relationships, and properties in AlzKB's knowledge
graph. Since databases are distributed in a variety of formats and
modalities, you will have to work with a mix of plain-text "flat"
files as well as relational (SQL) databases. All of the SQL databases
parsed to build AlzKB are distributed for MySQL (as opposed to some
other flavor of SQL).

** Flat file data sources

|-----------+----------------+-----------------------------------+---------------------------+--------------------|
| Source    | Directory name | Entity type(s)                    | URL                       | Extra instructions |
|-----------+----------------+-----------------------------------+---------------------------+--------------------|
| Hetionet  | =hetionet=     | Many - see =populate-ontology.py= | [[https://github.com/hetio/hetionet/tree/master/hetnet/tsv][GitHub]]                    | [[Hetionet]]           |
| NCBI Gene | =ncbigene=     | Genes                             | [[https://ftp.ncbi.nlm.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz][Homo_sapiens.gene_info.gz]] | [[NCBI Gene]]          |
| Drugbank  | =drugbank=     | Drugs / drug candidates           | [[https://go.drugbank.com/releases/latest#open-data][DrugBank website]]          | [[Drugbank]]           |
| DisGeNET  | =disgenet=     | Diseases and disease-gene edges   | [[https://www.disgenet.org/][DisGeNET]]                  | [[DisGeNET]]           |
|           |                |                                   |                           |                    |

*** Hetionet
Download the =hetionet-v1.0-edges.sif.gz= (extract it using =gunzip=)
and =hetionet-v1.0-nodes.tsv= files from the Hetionet Github
repository. Both of them are, essentially, TSV files, even though one
has the =.sif= extension.

Hetionet is, itself, a knowledge base, and contains many of the core
biological entities used in AlzKB. Accordingly, it contains data
derived from many other third-party sources.

*** NCBI Gene
Download the =Homo_sapiens.gene_info.gz= file from the NCBI FTP page
and extract it (e.g., using =gunzip=).

Create a =CUSTOM= subdirectory inside the =ncbigene= directory. Inside
of that subdirectory, place the following two files:
- [[https://github.com/EpistasisLab/AlzKB/blob/a9db2602e3e7960ec09749b99944fbf675323497/scripts/alzkb_parse_ncbigene.py][alzkb_parse_ncbigene.py]]
- [[https://bgee.org/ftp/bgee_v15_0/download/calls/expr_calls/Homo_sapiens_expr_advanced.tsv.gz][Homo_sapiens_expr_advanced.tsv]] (from the Bgee database)
Then, run =alzkb_parse_ncbigene.py= (no external Python packages
should be needed). You'll notice that it creates two output files
that are used while populating the ontology.

*** Drugbank
In order to download the Academic DrugBank datasets, you need to first create a free DrugBank account and verify your email address. After verifying your email address, they may need some more information regarding your DrugBank account, like the description of how you plan to use DrugBank, a description of your organization, Who is sponsoring this research, and What is the end goal of this research. Account approval can take up to several business days to weeks based on our experience. 

After your access has been approved, navigate to the Academic Download page on the Drugbank website (linked
above) by selecting the "Download" tab and "Academic Download". Select the "External Links" tab. In the table titled "External
Drug Links", click the "Download" button on the row labeled
"All". This will download a zip file. Extract the contents of that zip
file, and make sure it is named =drug_links.csv= (some versions use a
space instead of an underscore in the filename).

*** DisGeNET
Although DisGeNET is available under a Creative Commons license, the
database requires users to create a free account to download the
tab-delimited data files. Therefore, you should create a user account
and log in. Then, navigate to the Downloads page on the DisGeNET
website. Now, download the two necessary files by clicking on the
corresponding links:
- "UMLS CUI to several disease vocabularies" (under the "UMLS CUI to
  several disease vocabularies" section heading - the resulting file
  name will be =disease_mappings.tsv.gz=)
- "UMLS CUI to top disease classes" (the resulting file will be named
  =disease_mappings_to_attributes.tar.gz=)
Next, download =curated_disease_gene_associations.tsv.gz= directly by
copying the following URL into your web browser:
https://www.disgenet.org/static/disgenet_ap1/files/downloads/curated_gene_disease_associations.tsv.gz

All three files are gzipped, so extract them into the =disgenet/=
directory using your favorite method (e.g., gunzip from the command
line, 7zip from within Windows, etc.).

Now that you have the three necessary data files, you should run the
AlzKB script we wrote to filter for rows in those files corresponding
to Alzheimer's Disease, named =alzkb_parse_disgenet.py=. This script
is in the =scripts/= directory of the AlzKB repository, so either find
it on your local filesystem if you already have a copy of the
repository, or find it on the AlzKB GitHub repository in your web
browser.

You can then run the Python script from within the =disgenet/=
directory, which should deposit two filtered data files in the
=disgenet/CUSTOM/= subdirectory. These will be automatically detected
and used when you run the ontology population script, along with the
unmodified =curated_disease_gene_associations.tsv= file.

Then you create a directory that will hold all of the raw data files. It can be 'D:\data\' or something else you prefer. Within that, there will be 1 folder for each third-party database, and in those folders, you'll put the individual csv/tsv/txt files.

** SQL data sources
If you don't already have MySQL installed, install it. We recommend
using either a package manager (if one is available on your OS), or
installing MySQL Community Server from the mysql.com website (e.g., by
visiting https://dev.mysql.com/downloads/mysql/). Make sure it's
running and you have the ability to create and modify new databases.

*** AOP-DB 
The Adverse Outcome Pathway Database (AOP-DB) is the only MySQL
database you need to install to build the current version of AlzKB. It
can be downloaded at: https://gaftp.epa.gov/EPADataCommons/ORD/AOP-DB/

*WARNING:* This is a big download (7.2G while compressed)! Make sure
you have enough disk space before proceeding.

You'll have to extract two archives - first, unzip the =AOP-DB_v2.zip=
archive, which should contain two *.tar.gz archives and another .zip
archive. Now, extract the *.tar.gz archive containing =nogi= in its
name (the smaller of the two). Windows doesn't natively support
extracting .tar.gz archives, so you'll either have to download another
program that does this (e.g., 7-zip) or extract it in a Unix-based
environment (Linux, MacOS, Windows Subsystem for Linux, Cygwin, etc.)
that has the =tar= program available on the command line. Once you've
extracted it, you should have a file named something like
=aopdb_no-orthoscores.sql=.

Now, create an empty database in MySQL, and name it =aopdb=. Make sure
you have full admin privileges on the database. Then, load the (newly
extracted) =.sql= file into the empty database. I always find this
easiest from the command line, by running a command such as:
#+begin_src bash
  $ mysql -u username -p database_name < aopdb_no-orthoscores.sql
#+end_src
Substitute your username after the =-u= option and enter your password
when prompted. If you prefer to import it from a GUI, you can use a
tool like MySQL Workbench or DataGrip.

*WARNING:* It can take a while to import, so be ready to take a break
or do something else while you wait.

* 2.5: Populating the ontology
Now that we have an ontology (currently 'unpopulated', consisting of a
class hierarchy, object property types, data property types, and
possibly annotations), we can populate it with records from the
third-party databases we collected in the previous step. Fortunately,
this is a largely automated process, facilitated by a tool we call
=ista= (/ista/ is the Sindarin word for /knowledge/). With =ista=, you
write a Python script that first tells =ista= where to find the
third-party data sources, and then maps each of those data sources to
one or two node or edge types defined in the ontology (as classes or
object properties, respectively). Here, we'll walk through the
different parts of AlzKB's =ista= build script and discuss what each
component does. If you are reading this guide to modify or extend
AlzKB, you should be able to use the information in the following few
sections to write your own build script.

For reference, an up-to-date, complete copy of this build file can be
found in the [[https://github.com/EpistasisLab/AlzKB][AlzKB source repository]] at the location
=alzkb/populate_ontology.py=.

** Build file top-matter
At the top of the file, we do some imports of necessary Python
packages. First comes =ista=. We don't import the whole package, just
the classes and function that we actually interact with.
#+begin_src python
  from ista import FlatFileDatabaseParser, MySQLDatabaseParser
  from ista.util import print_onto_stats
#+end_src
In order to interact with OWL 2 ontology files, we bring in the
=owlready2= library.
#+begin_src python
  import owlready2
#+end_src
We put private data for our local MySQL databases (hostname, username,
and password) in a file named =secrets.py=, and then make sure the
file is added to our =.gitignore= file so it isn't checked into
version control. You'll have to create that file yourself, and define
the variables =MYSQL_HOSTNAME=, =MYSQL_USERNAME=, and
=MYSQL_PASSWORD=. Then, in the build script, you'll import the file
containing those variables and wrap them into a configuration dict.
#+begin_src python
  import secrets

  mysql_config = {
      'host': secrets.MYSQL_HOSTNAME,
      'user': secrets.MYSQL_USERNAME,
      'passwd': secrets.MYSQL_PASSWORD
  }
#+end_src
** Telling =ista= where to find your data sources
Since we are populating an ontology, we need to load the ontology into
=owlready2=. Make sure to modify this path to fit the location of the
AlzKB ontology file on your system! Future versions of AlzKB will
source the path dynamically. Also note the =file://= prefix, which
tells =owlready2= to look on the local file system rather than load a
web URL. Since this guide was made on a Windows desktop, you'll notice
that we have to use escaped backslashes to specify file paths that the
Python interpreter will parse correctly.
#+begin_src python
  onto = owlready2.get_ontology("file://D:\\projects\\ista\\tests\\projects\\alzkb\\alzkb.rdf").load()
#+end_src
We also set the 'base' directory for all of the flat files that =ista=
will be loading. You will have determined this location already (see
[[Obtaining the third-party data sources]]).
#+begin_src python
  data_dir = "D:\\data\\"
#+end_src
Now, we can actually register the source databases with =ista='s
parser classes. We use =FlatFileDatabaseParser= for data sources
stored as one or more delimited flat files, and =MySQLDatabaseParser=
for data sources in a MySQL database. For flat file-based sources, the
first argument given to the parser's constructor MUST be the
subdirectory (within =data_dir=) where that source's data files are
contained, and for MySQL sources it MUST be the name of the MySQL
database. If not, =ista= won't know where to find the files. The
second argument is always the ontology object loaded using
=owlready2=, and the third is either the base data directory or the
MySQL config dictionary, both of which were defined above.
#+begin_src python
  epa = FlatFileDatabaseParser("epa", onto, data_dir)
  ncbigene = FlatFileDatabaseParser("ncbigene", onto, data_dir)
  drugbank = FlatFileDatabaseParser("drugbank", onto, data_dir)
  hetionet = FlatFileDatabaseParser("hetionet", onto, data_dir)
  aopdb = MySQLDatabaseParser("aopdb", onto, mysql_config)
  aopwiki = FlatFileDatabaseParser("aopwiki", onto, data_dir)
  tox21 = FlatFileDatabaseParser("tox21", onto, data_dir)
  disgenet = FlatFileDatabaseParser("disgenet", onto, data_dir)
#+end_src
In the following two sections, we'll go over a few examples of how to
define mappings using these parser objects. We won't replicate every
mapping in this guide for brevity, but you can see all of them in the
full AlzKB build script.
*** Configuration for 'flat file' (e.g., CSV) data sources
#+begin_src python
hetionet.parse_node_type(
    node_type="Symptom",
    source_filename="hetionet-v1.0-nodes.tsv",
    fmt="tsv",
    parse_config={
        "iri_column_name": "name",
        "headers": True,
        "filter_column": "kind",
        "filter_value": "Symptom",
        "data_transforms": {
            "id": lambda x: x.split("::")[-1]
        },
        "data_property_map": {
            "id": onto.xrefMeSH,
            "name": onto.commonName
        }
    },
    merge=False,
    skip=False
)
#+end_src
This block indicates the third-party database is hetionet, and the file is hetionet-v1.0-nodes.tsv

So the file it will look for is D:\data\hetionet\hetionet-v1.0-nodes.tsv

Some of the configuration blocks will have a CUSTOM\ prefix to the filename. This means that the file was created by us manually and will need to be stored in a CUSTOM subdirectory of the database folder. For example:
#+begin_src python
disgenet.parse_node_type(
    node_type="Disease",
    source_filename="CUSTOM/disease_mappings_to_attributes_alzheimer.tsv",  # Filtered for just Alzheimer disease
    fmt="tsv-pandas",
    parse_config={
        "iri_column_name": "diseaseId",
        "headers": True,
        "data_property_map": {
            "diseaseId": onto.xrefUmlsCUI,
            "name": onto.commonName,
        }
    },
    merge=False,
    skip=False
)
#+end_src
This file will be D:\data\disgenet\CUSTOM\disease_mappings_alzheimer.tsv

*** Configuration for SQL server data sources
#+begin_src python
aopdb.parse_node_type(
    node_type="Drug",
    source_table="chemical_info",
    parse_config={
        "iri_column_name": "DTX_id",
        "data_property_map": {"ChemicalID": onto.xrefMeSH},
        "merge_column": {
            "source_column_name": "DTX_id",
            "data_property": onto.xrefDTXSID
        }
    },
    merge=True,
    skip=False
)
#+end_src
This block indicates the third-party database is AOP-DB, and the source table is chemical_info.


** Mapping data sources to ontology components
Every flat file or SQL table from a third-party data source can be
mapped a single node or relationship type. For example, a file
describing diseases can be mapped to the =Disease= node type, where
each line in the file corresponds to a disease to be inserted (or
'merged'---see below) into the knowledge graph. If the source is being
mapped to a node type (rather than a relationship type), =ista=
additionally can populate one or more /node properties/ from the
feature columns in the source file.

Each mapping is defined using a method call in the =ista= Python
script. 

** Running =ista=
Now you have set the location of data resources, ontology, and defined mapping method. Run populate_ontology.py 

The alzkb-populated.rdf is the output of this step and will be used for setting Neo4j Graph database.

* 3.: Converting the ontology into a Neo4j graph database

** Installing Neo4j
If you haven't done so already, download Neo4j from the [[https://neo4j.com/download-center/][Neo4j Download
Center]]. Most users should select Neo4j Desktop, but advanced users can
instead opt for Community Server (the instructions for which are well
outside of the scope of this guide).
** Configuring an empty graph database for AlzKB
You should now create a new graph database that will be populated with
the contents of AlzKB. In Neo4j Community, this can be done as follows:
- Create a new project by clicking the "New" button in the upper left,
  then selecting "Create project".
- In the project panel (on the right of the screen), you will see the
  default name "Project" populates automatically. Hover over this
  name and click the edit icon, then change the name to =AlzKB=.
- To the right of the project name, click "Add", and select "Local
  DBMS". Change the Name to =AlzKB DBMS=, specify a password that you will
  remember, and use the Version dropdown to select "4.4.0" (if it is
  not already selected). Click "Create". Wait for the operation to
  finish.
- Install plugins:
  - Click the name of the DBMS ("AlzKB DBMS", if you have followed the
    guide), and in the new panel to the right click the "Plugins" tab.
  - Expand the "APOC" option, click "Install", and wait for the
    operation to complete.
  - Do the same for the "Graph Data Science Library" and "Neosemantics
    (n10s)" plugins.
- Before starting the DBMS, click the ellipsis immediately to the
  right of the "Open" button, and then click "Settings...". Make the
  following changes to the configuration file:
  - Set =dbms.memory.heap.initial_size= to =2048m=.
  - Set =dbms.memory.heap.max_size= to =4G=.
  - Set =dbms.memory.pagecache.size= to =2048m=.
  - Uncomment the line containing
    =dbms.security.procedures.allowlist=apoc.coll.*,apoc.load.*,gds.*=
    to activate it.
  - Add =n10s.*,apoc.cypher.*,apoc.help=  to =dbms.security.procedures.allowlist=apoc.coll.*,apoc.load.*,gds.*=
  - Click the "Apply" button, then "Close".
- Click "Start" to start the graph database.
** Importing the =ista= RDF output into Neo4j
- Open neo4j Browser and run the following Cypher to import RDF data
#+begin_src cypher
   # Cleaning nodes
   MATCH (n) DETACH DELETE n
#+end_src

#+begin_src cypher
   # Constraint Creation
   CREATE CONSTRAINT n10s_unique_uri FOR (r:Resource) REQUIRE r.uri IS UNIQUE
#+end_src

#+begin_src cypher
   # Creating a Graph Configuration
   CALL n10s.graphconfig.init()
   CALL n10s.graphconfig.set({applyNeo4jNaming: true, handleVocabUris: 'IGNORE'})
#+end_src

#+begin_src cypher
   # Importing RDF
   CALL n10s.rdf.import.fetch( "file://D:\\data\\alzkb-populated.rdf", "RDF/XML")
#+end_src

- Run the Cyphers below to clean nodes
#+begin_src cypher
   MATCH (n:Resource) REMOVE n:Resource;
   MATCH (n:NamedIndividual) REMOVE n:NamedIndividual;
   MATCH (n:AllDisjointClasses) REMOVE n:AllDisjointClasses;
   MATCH (n:AllDisjointProperties) REMOVE n:AllDisjointProperties;
   MATCH (n:DatatypeProperty) REMOVE n:DatatypeProperty;
   MATCH (n:FunctionalProperty) REMOVE n:FunctionalProperty;
   MATCH (n:ObjectProperty) REMOVE n:ObjectProperty;
   MATCH (n:AnnotationProperty) REMOVE n:AnnotationProperty;
   MATCH (n:SymmetricProperty) REMOVE n:SymmetricProperty;
   MATCH (n:_GraphConfig) REMOVE n:_GraphConfig;
   MATCH (n:Ontology) REMOVE n:Ontology;
   MATCH (n:Restriction) REMOVE n:Restriction;
   MATCH (n:Class) REMOVE n:Class;
   MATCH (n) WHERE size(labels(n)) = 0 DETACH DELETE n; # Removes nodes without labels
#+end_src

Now, you have built the AlzKB from scratch. You can find the number of nodes and relationships with
#+begin_src cypher
CALL db.labels() YIELD label
CALL apoc.cypher.run('MATCH (:`'+label+'`) RETURN count(*) as count',{}) YIELD value
RETURN label, value.count ORDER BY label
#+end_src
#+begin_src cypher
CALL db.relationshipTypes() YIELD relationshipType as type
CALL apoc.cypher.run('MATCH ()-[:`'+type+'`]->() RETURN count(*) as count',{}) YIELD value
RETURN type, value.count ORDER BY type
#+end_src
